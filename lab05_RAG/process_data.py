"""
å‹å‹•åŸºæº–æ³•PDFè™•ç†èˆ‡RAGè³‡æ–™æº–å‚™ç¨‹å¼

æ­¤ç¨‹å¼å¯¦ç¾ä»¥ä¸‹åŠŸèƒ½ï¼š
1. è®€å–PDFå…§å®¹
2. æ™ºèƒ½åˆ†å‰²æ–‡æœ¬å…§å®¹
3. ç”Ÿæˆembeddingå‘é‡ï¼ˆå¤šåŸ·è¡Œç·’ä¸¦è¡Œè™•ç†ï¼‰
4. å„²å­˜è‡³PostgreSQLè³‡æ–™åº«

ä¸»è¦ä¾è³´å¥—ä»¶ï¼š
- PyPDF2: PDFè®€å–
- langchain: æ–‡æœ¬åˆ†å‰²
- psycopg2: PostgreSQLé€£æ¥
- numpy: æ•¸å€¼è¨ˆç®—
- threading: å¤šåŸ·è¡Œç·’è™•ç†
"""

import os
import re
import hashlib
from datetime import datetime
from typing import List, Dict, Tuple
import psycopg2
from psycopg2.extras import execute_values
import numpy as np
from dotenv import load_dotenv
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from utils.database_config import get_database_config
from utils.ai_client import get_embedding_for_content

# PDFè™•ç†ç›¸é—œ
try:
    import PyPDF2
except ImportError:
    print("è«‹å®‰è£PyPDF2: pip install PyPDF2")

# æ–‡æœ¬åˆ†å‰²ç›¸é—œ
try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
except ImportError:
    print("è«‹å®‰è£langchain: pip install langchain")

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

class LaborLawProcessor:
    """å‹å‹•åŸºæº–æ³•PDFè™•ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è™•ç†å™¨"""
        # æ–‡æœ¬åˆ†å‰²å™¨é…ç½®
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=400,         # æ¯æ®µæ–‡å­—ç´„400å­—ç¬¦
            chunk_overlap=200,       # æ®µè½é–“é‡ç–Š200å­—ç¬¦ï¼Œç¢ºä¿ä¸Šä¸‹æ–‡é€£è²«
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼›", "ï¼Œ", " ", ""]  # æŒ‰å„ªå…ˆé †åºåˆ†å‰²
        )
        
        # PostgreSQLé€£æ¥é…ç½®
        self.db_config = get_database_config()
        
        # å¤šåŸ·è¡Œç·’é…ç½®
        self.max_workers = 4  # ä½¿ç”¨4å€‹åŸ·è¡Œç·’
        self.lock = threading.Lock()  # åŸ·è¡Œç·’é–ç”¨æ–¼å®‰å…¨è¼¸å‡º
    
    def read_pdf(self, pdf_path: str) -> str:
        """
        è®€å–PDFæ–‡ä»¶å…§å®¹
        
        Args:
            pdf_path (str): PDFæª”æ¡ˆè·¯å¾‘
            
        Returns:
            str: PDFæ–‡æœ¬å…§å®¹
        """
        print(f"æ­£åœ¨è®€å–PDFæª”æ¡ˆ: {pdf_path}")
        
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                # æå–æ‰€æœ‰é é¢çš„æ–‡å­—
                text_content = ""
                total_pages = len(pdf_reader.pages)
                
                for page_num in range(total_pages):
                    page = pdf_reader.pages[page_num]
                    page_text = page.extract_text()
                    text_content += page_text + "\n"
                    
                    # é¡¯ç¤ºé€²åº¦
                    print(f"å·²è®€å– {page_num + 1}/{total_pages} é ")
                
                print(f"PDFè®€å–å®Œæˆï¼Œå…± {total_pages} é ï¼Œç¸½å­—æ•¸: {len(text_content)}")
                return text_content
                
        except Exception as e:
            print(f"è®€å–PDFæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return ""
    
    def query_aoai_embedding(self, content: str) -> list[float]:
        """å¾ Azure OpenAI æœå‹™ç²å–æ–‡æœ¬çš„ embedding å‘é‡

        Args:
            content (str): è¦é€²è¡Œ embedding çš„æ–‡æœ¬å…§å®¹

        Returns:
            list[float]: è¿”å› embedding å‘é‡ï¼Œå¦‚æœç™¼ç”ŸéŒ¯èª¤å‰‡è¿”å›ç©ºåˆ—è¡¨
        """
        return get_embedding_for_content(content)


    def preprocess_text(self, text: str) -> str:
        """
        é è™•ç†æ–‡æœ¬å…§å®¹
        
        Args:
            text (str): åŸå§‹æ–‡æœ¬
            
        Returns:
            str: é è™•ç†å¾Œçš„æ–‡æœ¬
        """
        print("æ­£åœ¨é€²è¡Œæ–‡æœ¬é è™•ç†...")
        
        # ç§»é™¤å¤šé¤˜çš„ç©ºç™½å’Œæ›è¡Œ
        text = re.sub(r'\n+', '\n', text)
        text = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤é ç¢¼å’Œé çœ‰é è…³
        text = re.sub(r'ç¬¬\s*\d+\s*é ', '', text)
        text = re.sub(r'- \d+ -', '', text)
        
        # æ­£è¦åŒ–æ¨™é»ç¬¦è™Ÿ
        text = text.replace('ã€€', ' ')  # å°‡å…¨å½¢ç©ºæ ¼æ›¿æ›ç‚ºåŠå½¢ç©ºæ ¼
        
        print("æ–‡æœ¬é è™•ç†å®Œæˆ")
        return text.strip()
    
    def split_text_intelligently(self, text: str) -> List[Dict]:
        """
        æ™ºèƒ½åˆ†å‰²æ–‡æœ¬ï¼Œä¿ç•™æ³•æ¢çµæ§‹
        
        Args:
            text (str): é è™•ç†å¾Œçš„æ–‡æœ¬
            
        Returns:
            List[Dict]: åˆ†å‰²å¾Œçš„æ–‡æœ¬æ®µè½åˆ—è¡¨ï¼ŒåŒ…å«å…ƒæ•¸æ“š
        """
        print("æ­£åœ¨é€²è¡Œæ™ºèƒ½æ–‡æœ¬åˆ†å‰²...")
        
        # ä½¿ç”¨RecursiveCharacterTextSplitteré€²è¡ŒåŸºæœ¬åˆ†å‰²
        chunks = self.text_splitter.split_text(text)
        
        # ç‚ºæ¯å€‹chunkæ·»åŠ å…ƒæ•¸æ“š
        processed_chunks = []
        
        for i, chunk in enumerate(chunks):
            # æª¢æ¸¬æ˜¯å¦ç‚ºæ³•æ¢
            article_match = re.search(r'ç¬¬\s*(\d+)\s*æ¢', chunk)
            chapter_match = re.search(r'ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)\s*ç« ', chunk)
            
            # ç”Ÿæˆå…§å®¹contextè³‡è¨Š
            context = f"æ®µè½{i+1}"
            if article_match:
                context += f" | ç¬¬{article_match.group(1)}æ¢"
            if chapter_match:
                context += f" | ç¬¬{chapter_match.group(1)}ç« "
            
            chunk_data = {
                'content': chunk.strip(),
                'context': context,
                'chunk_index': i,
                'article_number': article_match.group(1) if article_match else None,
                'chapter_info': chapter_match.group(1) if chapter_match else None,
                'char_count': len(chunk)
            }
            
            processed_chunks.append(chunk_data)
        
        print(f"æ–‡æœ¬åˆ†å‰²å®Œæˆï¼Œå…±ç”Ÿæˆ {len(processed_chunks)} å€‹æ®µè½")
        return processed_chunks
    
    def process_embedding_batch(self, chunks_batch: List[Dict], batch_id: int, total_batches: int) -> List[Dict]:
        """
        è™•ç†ä¸€æ‰¹embeddingï¼ˆå–®ä¸€åŸ·è¡Œç·’ä»»å‹™ï¼‰
        
        Args:
            chunks_batch (List[Dict]): æ–‡æœ¬æ®µè½æ‰¹æ¬¡
            batch_id (int): æ‰¹æ¬¡ç·¨è™Ÿ
            total_batches (int): ç¸½æ‰¹æ¬¡æ•¸
            
        Returns:
            List[Dict]: è™•ç†å®Œæˆçš„æ®µè½åˆ—è¡¨
        """
        processed_chunks = []
        
        for i, chunk in enumerate(chunks_batch):
            try:
                # ç”Ÿæˆembedding
                embedding = self.query_aoai_embedding(chunk['content'])
                chunk['embedding'] = embedding
                
                # åŸ·è¡Œç·’å®‰å…¨çš„é€²åº¦è¼¸å‡º
                with self.lock:
                    print(f"ğŸ§µ åŸ·è¡Œç·’ {batch_id}/{total_batches} | æ®µè½ {i+1}/{len(chunks_batch)}")
                
                processed_chunks.append(chunk)
                
                # é¿å…APIè«‹æ±‚éæ–¼é »ç¹
                time.sleep(0.1)
                
            except Exception as e:
                with self.lock:
                    print(f"âŒ åŸ·è¡Œç·’ {batch_id} è™•ç†æ®µè½ {i+1} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                chunk['embedding'] = None
                processed_chunks.append(chunk)
        
        with self.lock:
            print(f"âœ… åŸ·è¡Œç·’ {batch_id} å®Œæˆï¼Œè™•ç†äº† {len(chunks_batch)} å€‹æ®µè½")
        
        return processed_chunks
    
    def generate_embeddings(self, chunks: List[Dict]) -> List[Dict]:
        """
        ä½¿ç”¨å¤šåŸ·è¡Œç·’ç‚ºæ–‡æœ¬æ®µè½ç”Ÿæˆembeddingå‘é‡
        
        Args:
            chunks (List[Dict]): æ–‡æœ¬æ®µè½åˆ—è¡¨
            
        Returns:
            List[Dict]: åŒ…å«embeddingçš„æ®µè½åˆ—è¡¨
        """
        print(f"ğŸš€ é–‹å§‹ä½¿ç”¨ {self.max_workers} å€‹åŸ·è¡Œç·’ç”Ÿæˆembeddingå‘é‡...")
        print(f"ğŸ“Š ç¸½å…±éœ€è¦è™•ç† {len(chunks)} å€‹æ®µè½")
        
        # å°‡chunksåˆ†å‰²æˆ4å€‹æ‰¹æ¬¡
        batch_size = len(chunks) // self.max_workers
        if len(chunks) % self.max_workers != 0:
            batch_size += 1
        
        batches = []
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            batches.append(batch)
        
        print(f"ğŸ“¦ åˆ†å‰²æˆ {len(batches)} å€‹æ‰¹æ¬¡ï¼Œæ¯æ‰¹æ¬¡ç´„ {batch_size} å€‹æ®µè½")
        
        # è¨˜éŒ„é–‹å§‹æ™‚é–“
        start_time = time.time()
        
        # ä½¿ç”¨ThreadPoolExecutoré€²è¡Œå¤šåŸ·è¡Œç·’è™•ç†
        all_processed_chunks = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_batch = {
                executor.submit(self.process_embedding_batch, batch, i+1, len(batches)): i 
                for i, batch in enumerate(batches)
            }
            
            # æ”¶é›†çµæœ
            for future in as_completed(future_to_batch):
                batch_id = future_to_batch[future]
                try:
                    batch_results = future.result()
                    all_processed_chunks.extend(batch_results)
                except Exception as e:
                    print(f"âŒ æ‰¹æ¬¡ {batch_id + 1} è™•ç†å¤±æ•—: {e}")
        
        # æŒ‰åŸå§‹é †åºæ’åº
        all_processed_chunks.sort(key=lambda x: x['chunk_index'])
        
        # è¨ˆç®—è™•ç†æ™‚é–“
        end_time = time.time()
        processing_time = end_time - start_time
        
        # çµ±è¨ˆæˆåŠŸ/å¤±æ•—æ•¸é‡
        successful_embeddings = sum(1 for chunk in all_processed_chunks if chunk.get('embedding') is not None)
        failed_embeddings = len(all_processed_chunks) - successful_embeddings
        
        print("=" * 60)
        print(f"ğŸ‰ å¤šåŸ·è¡Œç·’Embeddingç”Ÿæˆå®Œæˆï¼")
        print(f"â±ï¸  ç¸½è™•ç†æ™‚é–“: {processing_time:.2f} ç§’")
        print(f"âœ… æˆåŠŸç”Ÿæˆ: {successful_embeddings} å€‹embedding")
        print(f"âŒ å¤±æ•—: {failed_embeddings} å€‹embedding")
        print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {successful_embeddings/processing_time:.2f} å€‹embedding/ç§’")
        print("=" * 60)
        
        return all_processed_chunks
    
    def create_database_tables(self):
        """å‰µå»ºPostgreSQLè³‡æ–™è¡¨"""
        print("æ­£åœ¨å‰µå»ºè³‡æ–™åº«è¡¨æ ¼...")
        
        try:
            conn = psycopg2.connect(**self.db_config)
            cur = conn.cursor()
            
            print("âœ… ä½¿ç”¨embeddingsè¡¨æ ¼çµæ§‹")
            
            embeddings_table = """
            CREATE TABLE IF NOT EXISTS public.embeddings
            (
                id SERIAL NOT NULL,
                embedding_vector double precision[],
                created_at timestamp without time zone DEFAULT CURRENT_TIMESTAMP,
                content text COLLATE pg_catalog."default",
                context text NOT NULL,
                CONSTRAINT embeddings_pkey PRIMARY KEY (id)
            );
            """
            
            # å‰µå»ºåŸºæœ¬ç´¢å¼•ä»¥æé«˜æŸ¥è©¢æ•ˆèƒ½
            indexes = """
            CREATE INDEX IF NOT EXISTS idx_embeddings_content ON embeddings USING gin(to_tsvector('chinese', content));
            CREATE INDEX IF NOT EXISTS idx_embeddings_context ON embeddings(context);
            """
            
            # å‰µå»ºè¡¨æ ¼å’Œç´¢å¼•
            cur.execute(embeddings_table)
            cur.execute(indexes)
            
            conn.commit()
            cur.close()
            conn.close()
            
            print("âœ… è³‡æ–™åº«è¡¨æ ¼å‰µå»ºå®Œæˆ")
            print("ğŸ” æ”¯æ´å…¨æ–‡æª¢ç´¢å’Œå‘é‡ç›¸ä¼¼åº¦æœç´¢åŠŸèƒ½")
            
        except Exception as e:
            print(f"âŒ å‰µå»ºè³‡æ–™åº«è¡¨æ ¼æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            print("\nğŸ”§ å¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆï¼š")
            print("1. ç¢ºèªPostgreSQLæœå‹™æ­£åœ¨é‹è¡Œ")
            print("2. æª¢æŸ¥è³‡æ–™åº«é€£æ¥è¨­å®š")
            print("3. ç¢ºèªè³‡æ–™åº«æ¬Šé™")
    
    def save_to_database(self, chunks: List[Dict]):
        """
        å°‡è™•ç†å¾Œçš„è³‡æ–™å„²å­˜åˆ°PostgreSQL
        
        Args:
            chunks (List[Dict]): åŒ…å«embeddingçš„æ–‡æœ¬æ®µè½åˆ—è¡¨
        """
        print("æ­£åœ¨å„²å­˜è³‡æ–™åˆ°PostgreSQL...")
        
        try:
            conn = psycopg2.connect(**self.db_config)
            cur = conn.cursor()
            
            # æº–å‚™æ‰¹é‡æ’å…¥è³‡æ–™
            embedding_data = []
            
            for chunk in chunks:
                if chunk.get('embedding'):
                    embedding_data.append((
                        chunk['embedding'],          # embedding_vector
                        chunk['content'],           # content
                        chunk['context']            # context
                    ))
            
            # æ‰¹é‡æ’å…¥embeddingè³‡æ–™
            execute_values(cur, """
                INSERT INTO embeddings (embedding_vector, content, context)
                VALUES %s
            """, embedding_data)
            
            conn.commit()
            cur.close()
            conn.close()
            
            print(f"âœ… è³‡æ–™å„²å­˜å®Œæˆï¼å…±å„²å­˜ {len(embedding_data)} ç­†è¨˜éŒ„åˆ°embeddingsè¡¨æ ¼")
            
        except Exception as e:
            print(f"âŒ å„²å­˜è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def check_existing_data(self) -> int:
        """
        æª¢æŸ¥è³‡æ–™åº«ä¸­ç¾æœ‰çš„è³‡æ–™æ•¸é‡
        
        Returns:
            int: ç¾æœ‰è¨˜éŒ„æ•¸é‡
        """
        try:
            conn = psycopg2.connect(**self.db_config)
            cur = conn.cursor()
            
            cur.execute("SELECT COUNT(*) FROM embeddings")
            count = cur.fetchone()[0]
            
            cur.close()
            conn.close()
            
            return count
            
        except Exception as e:
            print(f"æª¢æŸ¥ç¾æœ‰è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return 0
    
    def process_pdf(self, pdf_path: str):
        """
        å®Œæ•´çš„PDFè™•ç†æµç¨‹
        
        Args:
            pdf_path (str): PDFæª”æ¡ˆè·¯å¾‘
        """
        print("=" * 50)
        print("é–‹å§‹è™•ç†å‹å‹•åŸºæº–æ³•PDF")
        print("=" * 50)
        
        # æ­¥é©Ÿ1ï¼šå‰µå»ºè³‡æ–™åº«è¡¨æ ¼
        self.create_database_tables()
        
        # æ­¥é©Ÿ2ï¼šæª¢æŸ¥ç¾æœ‰è³‡æ–™
        existing_count = self.check_existing_data()
        if existing_count > 0:
            print(f"âš ï¸  è³‡æ–™åº«ä¸­å·²æœ‰ {existing_count} ç­†è¨˜éŒ„")
            response = input("æ˜¯å¦è¦æ¸…é™¤ç¾æœ‰è³‡æ–™ä¸¦é‡æ–°è™•ç†ï¼Ÿ(y/N): ")
            if response.lower() == 'y':
                try:
                    conn = psycopg2.connect(**self.db_config)
                    cur = conn.cursor()
                    cur.execute("DELETE FROM embeddings")
                    conn.commit()
                    cur.close()
                    conn.close()
                    print("âœ… å·²æ¸…é™¤ç¾æœ‰è³‡æ–™")
                except Exception as e:
                    print(f"âŒ æ¸…é™¤è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    return
            else:
                print("å–æ¶ˆè™•ç†")
                return
        
        # æ­¥é©Ÿ3ï¼šè®€å–PDFå…§å®¹
        raw_text = self.read_pdf(pdf_path)
        if not raw_text:
            print("PDFè®€å–å¤±æ•—ï¼Œç¨‹å¼çµ‚æ­¢")
            return
        
        # æ­¥é©Ÿ4ï¼šé è™•ç†æ–‡æœ¬
        cleaned_text = self.preprocess_text(raw_text)
        
        # æ­¥é©Ÿ5ï¼šæ™ºèƒ½åˆ†å‰²æ–‡æœ¬
        chunks = self.split_text_intelligently(cleaned_text)
        
        # æ­¥é©Ÿ6ï¼šç”Ÿæˆembeddingå‘é‡
        chunks_with_embeddings = self.generate_embeddings(chunks)
        
        # æ­¥é©Ÿ7ï¼šå„²å­˜åˆ°è³‡æ–™åº«
        self.save_to_database(chunks_with_embeddings)
        
        print("=" * 50)
        print("PDFè™•ç†å®Œæˆï¼")
        print("=" * 50)
        
        # é¡¯ç¤ºæœ€çµ‚çµ±è¨ˆ
        final_count = self.check_existing_data()
        print(f"ğŸ“Š è³‡æ–™åº«ä¸­å…±æœ‰ {final_count} ç­†embeddingè¨˜éŒ„")

def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    # åˆå§‹åŒ–è™•ç†å™¨
    processor = LaborLawProcessor()
    
    # PDFæª”æ¡ˆè·¯å¾‘
    pdf_path = "å‹å‹•åŸºæº–æ³•.pdf"
    
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists(pdf_path):
        print(f"æ‰¾ä¸åˆ°PDFæª”æ¡ˆ: {pdf_path}")
        return
    
    # é–‹å§‹è™•ç†
    processor.process_pdf(pdf_path)

if __name__ == "__main__":
    main()
